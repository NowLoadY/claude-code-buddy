#!/usr/bin/env tsx
import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs';
import Anthropic from '@anthropic-ai/sdk';
import OpenAI from 'openai';
import * as readline from 'readline';

const execAsync = promisify(exec);
const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY! });
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

const conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }> = [];

async function recordAudio(outputPath: string, duration: number = 10): Promise<void> {
  console.log('\nğŸ”´ éŒ„éŸ³ä¸­... è«‹èªªè©± (æŒ‰ Ctrl+C å¯éš¨æ™‚åœæ­¢)');
  await execAsync(`ffmpeg -f avfoundation -i ":0" -t ${duration} -y "${outputPath}" 2>&1 > /dev/null`);
  console.log('âœ… éŒ„éŸ³å®Œæˆ\n');
}

async function transcribe(audioPath: string): Promise<string> {
  const transcription = await openai.audio.transcriptions.create({
    file: fs.createReadStream(audioPath),
    model: 'whisper-1',
  });
  return transcription.text;
}

async function chat(userMessage: string): Promise<string> {
  conversationHistory.push({ role: 'user', content: userMessage });

  const message = await anthropic.messages.create({
    model: 'claude-sonnet-4-5-20250929',
    max_tokens: 2048,
    messages: conversationHistory
  });

  const response = message.content[0];
  const responseText = response.type === 'text' ? response.text : '';

  conversationHistory.push({ role: 'assistant', content: responseText });

  return responseText;
}

async function speak(text: string, outputPath: string): Promise<void> {
  const mp3 = await openai.audio.speech.create({
    model: 'tts-1-hd',
    voice: 'nova',
    input: text,
    speed: 1.1
  });
  const buffer = Buffer.from(await mp3.arrayBuffer());
  await fs.promises.writeFile(outputPath, buffer);
  await execAsync(`afplay "${outputPath}"`);
}

async function askToContinue(): Promise<boolean> {
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout
  });

  return new Promise((resolve) => {
    rl.question('\nğŸ™ï¸  æŒ‰ ENTER ç¹¼çºŒå°è©±ï¼Œæˆ–è¼¸å…¥ "quit" é€€å‡º: ', (answer) => {
      rl.close();
      resolve(answer.trim().toLowerCase() !== 'quit');
    });
  });
}

async function main() {
  console.clear();
  console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
  console.log('â•‘   ğŸ™ï¸  Interactive Voice Chat with Claude    â•‘');
  console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
  console.log('ğŸ’¡ æ¯æ¬¡éŒ„éŸ³ 10 ç§’ï¼Œè«‹æº–å‚™å¥½å¾ŒæŒ‰ ENTER é–‹å§‹\n');

  const recordingPath = '/tmp/recording.wav';
  const responsePath = '/tmp/response.mp3';

  // åˆå§‹å•å€™
  console.log('ğŸ¤– Claude: ä½ å¥½ï¼æˆ‘æ˜¯ Smart Agents èªéŸ³åŠ©æ‰‹ï¼Œæœ‰ä»€éº¼å¯ä»¥å¹«ä½ çš„å—ï¼Ÿ');
  await speak('ä½ å¥½ï¼æˆ‘æ˜¯ Smart Agents èªéŸ³åŠ©æ‰‹ï¼Œæœ‰ä»€éº¼å¯ä»¥å¹«ä½ çš„å—ï¼Ÿ', responsePath);

  // å°è©±å¾ªç’°
  while (true) {
    const shouldContinue = await askToContinue();
    if (!shouldContinue) {
      console.log('\nğŸ‘‹ å†è¦‹ï¼\n');
      break;
    }

    try {
      // éŒ„éŸ³
      await recordAudio(recordingPath, 10);

      // è½‰æ–‡å­—
      const userText = await transcribe(recordingPath);
      console.log('ğŸ‘¤ ä½ : ' + userText + '\n');

      // Claude å›æ‡‰
      const response = await chat(userText);
      console.log('ğŸ¤– Claude: ' + response);

      // èªéŸ³æ’­æ”¾
      await speak(response, responsePath);

    } catch (error: any) {
      console.error('âŒ éŒ¯èª¤:', error.message);
      console.log('è®“æˆ‘å€‘é‡è©¦...\n');
    }
  }
}

main();
